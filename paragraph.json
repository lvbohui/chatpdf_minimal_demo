{
    "p1.txt": "With the presence of large text-to-image models, generating a visually appealing image may require\nonly a short descriptive prompt entered by users. After typing some texts and getting the images, we\nmay naturally come up with several questions: does this prompt-based control satisfy our needs? For\nexample in image processing, considering many long-standing tasks with clear problem formulations,\ncan these large models be applied to facilitate these specific tasks? What kind of framework should\nwe build to handle the wide range of problem conditions and user controls? In specific tasks, can\nlarge models preserve the advantages and capabilities obtained from billions of images?\nTo answer these questions, we investigate various image processing applications and have three\nfindings. First, the available data scale in a task-specific domain is not always as large as that in\nthe general image-text domain. The largest dataset size of many specific problems (e.g., object\nshape/normal, pose understanding, etc.) is often under 100k, i.e., 5 \u00d7 10 4 times smaller than LAION-\n5B. This would require robust neural network training method to avoid overfitting and to preserve\ngeneralization ability when the large models are trained for specific problems.\nSecond, when image processing tasks are handled with data-driven solutions, large computation\nclusters are not always available. This makes fast training methods important for optimizing large\nmodels to specific tasks within an acceptable amount of time and memory space (e.g., on personal\ndevices). This would further require the utilization of pretrained weights, as well as fine-tuning\nstrategies or transfer learning.\nThird, various image processing problems have diverse forms of problem definitions, user controls,\nor image annotations. When addressing these problems, although an image diffusion algorithm can\nbe regulated in a \u201cprocedural\u201d way, e.g., constraining denoising process, editing multi-head attention\nactivations, etc., the behaviors of these hand-crafted rules are fundamentally prescribed by human\ndirectives. Considering some specific tasks like depth-to-image, pose-to-human, etc., these problems\nessentially require the interpretation of raw inputs into object-level or scene-level understandings,\nmaking hand-crafted procedural methods less feasible. To achieve learned solutions in many tasks,\nthe end-to-end learning is indispensable.\nThis paper presents ControlNet, an end-to-end neural network architecture that controls large image\ndiffusion models (like Stable Diffusion) to learn task-specific input conditions. The ControlNet clones\nthe weights of a large diffusion model into a \"trainable copy\" and a \"locked copy\": the locked copy\npreserves the network capability learned from billions of images, while the trainable copy is trained\non task-specific datasets to learn the conditional control. The trainable and locked neural network\nblocks are connected with an unique type of convolution layer called \"zero convolution\", where the\nconvolution weights progressively grow from zeros to optimized parameters in a learned manner.\nSince the production-ready weights are preserved, the training is robust at datasets of different scale.\nSince the zero convolution does not add new noise to deep features, the training is as fast as fine\ntuning a diffusion model, compared to training new layers from scratch.\nWe train several ControlNets with various datasets of different conditions, e.g., Canny edges, Hough\nlines, user scribbles, human key points, segmentation maps, shape normals, depths, etc. We also\nexperiment ControlNets with both small datasets (with samples less than 50k or even 1k) and\nlarge datasets (millions of samples). We also show that in some tasks like depth-to-image, training\nControlNets on a personal computer (one Nvidia RTX 3090TI) can achieve competitive results\nto commercial models trained on large computation clusters with terabytes of GPU memory and\nthousands of GPU hours.",
    "p2.txt": "2.1\nHyperNetwork and Neural Network Structure\nHyperNetwork originates from a neural language processing method [14] to train a small recurrent\nneural network to influence the weights of a larger one. Successful results of HyperNetwork are\nalso reported in image generation using generative adversarial networks [1, 10] and other machine\nlearning tasks [51]. Inspired by these ideas, [15] provided a method to attach a smaller neural network\nto Stable Diffusion [44] so as to change the artistic style of its output images. This approach gained\nmore popularity after [28] provided the pretrained weights of several HyperNetworks. ControlNet\nand HyperNetwork have similarities in the way they influence the behaviors of neural networks.\nControlNet uses a special type of convolution layer called \u201czero convolution\u201d. Early neural network\nstudies [31, 47, 32] have extensively discussed the initialization of network weights, including the\nrationality of initializing the weights with Gaussian distributions and the risks that may incur by\ninitializing the weights with zeros. More recently, [37] discussed a method to scale the initial weight\nof several convolution layers in a diffusion model to improve the training, which shares similarity with\nthe idea of zero convolution (and their codes contain a function called \u201czero_module\u201d). Manipulating\nthe initial convolution weights is also discussed in ProGAN [21] and StyleGAN [22], as well as\nNoise2Noise [33] and [65]. Stability\u2019s model cards [55] also mention the use of zero weights in\nneural layers.",
    "p3.txt": "2.2\nDiffusion Probabilistic Model\nDiffusion probabilistic model was proposed in [52]. Successful results of image generation are first\nreported at small scale [25] and then relatively large scale [9]. This architecture was improved by\nimportant training and sampling methods like Denoising Diffusion Probabilistic Model (DDPM) [17],\nDenoising Diffusion Implicit Model (DDIM) [53], and score-based diffusion [54]. Image diffusion\nmethods can directly use pixel colors as training data, and in that case, researches often consider\nstrategies to save computation powers when handling high-resolution images [53, 50, 26], or directly\nuse pyramid-based or multiple-stage methods [18, 43]. These methods essentially use U-net [45] as\ntheir neural network architecture. In order to reduce the computation power required for training a\ndiffusion model, based on the idea of latent image [11], the approach Latent Diffusion Model (LDM)\n[44] was proposed and further extended to Stable Diffusion.",
    "p4.txt": "2.3\nText-to-Image Diffusion\nDiffusion models can be applied to text-to-image generating tasks to achieve state-of-the-art image\ngenerating results. This is often achieved by encoding text inputs into latent vectors using pretrained\nlanguage models like CLIP [41]. For instances, Glide [38] is a text-guided diffusion models supporting\nboth image generating and editing. Disco Diffusion is a clip-guided implementation of [9] to process\ntext prompts. Stable Diffusion is a large scale implementation of latent diffusion [44] to achieve\ntext-to-image generation. Imagen [49] is a text-to-image structure that does not use latent images and\ndirectly diffuse pixels using a pyramid structure.",
    "p5.txt": "2.4\nPersonalization,Customization, and Control of Pretrained Diffusion Model\nBecause state-of-the-art image diffusion models are dominated by text-to-image methods, the most\nstraight-forward ways to enhance the control over a diffusion model are often text-guided [38, 24,\n2, 3, 23, 43, 16]. This type of control can also be achieved by manipulating CLIP features [43].\nThe image diffusion process by itself can provide some functionalities to achieve color-level detail\nvariations [35] (the community of Stable Diffusion call it img2img). Image diffusion algorithms\nnaturally supports inpainting as an important way to control the results [43, 2]. Textual Inversion\n[12] and DreamBooth [46] are proposed to customize (or personalize) the contents in the generated\nresults using a small set of images with same topics or objects.",
    "p6.txt": "2.5\nImage-to-Image Translation\nWe would like to point out that, although the ControlNet and image-to-image translation may have\nseveral overlapped applications, their motivations are essentially different. Image-to-image translation\nis targeted to learn a mapping between images in different domains, while a ControlNet is targeted to\ncontrol a diffusion model with task-specific conditions.\nPix2Pix [20] presented the concept of image-to-image translation, and early methods are dominated\nby conditional generative neural networks [20, 69, 60, 39, 8, 63, 68]. After transformers and Vision\nTransformers (ViTs) gained popularity, successful results have been reported using autoregressive\nmethods [42, 11, 7]. Some researches also show that multi-model methods can learn a robust\ngenerator from various translation tasks [64, 29, 19, 40].\nWe discuss the current strongest methods in image-to-image translation. Taming Transformer [11]\nis a vision transformer with the capability to both generate images and perform image-to-image\ntranslations. Palette [48] is an unified diffusion-based image-to-image translation framework. PITI\n[59] is a diffusion-based image-to-image translation method that utilizes large-scale pretraining as a\nway to improve the quality of generated results. In specific fields like sketch-guided diffusion, [58] is\na optimization-based method that manipulates the diffusion process. These methods are tested in the\nexperiments.",
    "p7.txt": "3 Method\nControlNet is a neural network architecture that can enhance pretrained image diffusion models\nwith task-specific conditions. We introduce ControlNet\u2019s essential structure and motivate of each\npart in Section 3.1. We detail the method to apply ControlNets to image diffusion models using the\nexample of Stable Diffusion in Section 3.2. We elaborate the learning objective and general training\nmethod in Section 3.3, and then describe several approaches to improve the training in extreme\ncases such as training with one single laptop or using large-scale computing clusters in Section 3.4.\nFinally, we include the details of several ControlNet implementations with different input conditions\nin Section 3.5.",
    "p8.txt": "3.1\nControlNet\nControlNet manipulates the input conditions of neural network blocks so as to further control the\noverall behavior of an entire neural network. Herein, a \"network block\" refers to a set of neural\nlayers that are put together as a frequently used unit to build neural networks, e.g., \u201cresnet\u201d block,\n\u201cconv-bn-relu\u201d block, multi-head attention block, transformer block, etc.\nUsing 2D feature as an example, given a feature map x \u2208 R h\u00d7w\u00d7c with {h, w, c} being height,\nwidth, and channel numbers, a neural network block F(\u00b7; \u0398) with a set of parameters \u0398 transforms x\ninto another feature map y with\ny = F(x; \u0398)\n(1)\nand this procedure is visualized in Fig. 2-(a).\nWe lock all parameters in \u0398 and then clone it into a trainable copy \u0398 c . The copied \u0398 c is trained with\nan external condition vector c. In this paper, we call the original and new parameters \u201clocked copy\u201d\nand \u201ctrainable copy\u201d. The motivation of making such copies rather than directly training the original\nweights is to avoid overfitting when dataset is small and to preserve the production-ready quality of\nlarge models learned from billions of images.\nThe neural network blocks are connected by an unique type of convolution layer called \u201czero\nconvolution\u201d, i.e., 1 \u00d7 1 convolution layer with both weight and bias initialized with zeros. We denote\nthe zero convolution operation as Z(\u00b7; \u00b7) and use two instances of parameters {\u0398 z1 , \u0398 z2 } to compose\nthe ControlNet structure with\ny c = F(x; \u0398) + Z(F(x + Z(c; \u0398 z1 ); \u0398 c ); \u0398 z2 )\n(2)\nwhere y c becomes the output of this neural network block, as visualized in Fig. 2-(b).\nBecause both the weight and bias of a zero convolution layer are initialized as zeros, in the first\ntraining step, we have\n\uf8f1\n\uf8f2 Z(c; \u0398 z1 ) = 0\nF(x + Z(c; \u0398 z1 ); \u0398 c ) = F(x; \u0398 c ) = F(x; \u0398)\n(3)\n\uf8f3\nZ(F(x + Z(c; \u0398 z1 ); \u0398 c ); \u0398 z2 ) = Z(F(x; \u0398 c ); \u0398 z2 ) = 0\nand this can be converted to\ny c = y\n(4)\nand Eq-(2,3,4) indicate that, in the first training step, all the inputs and outputs of both the trainable and\nlocked copy of neural network blocks are consistent with what they would be as if the ControlNet does\nnot exist. In other words, when a ControlNet is applied to some neural network blocks, before any\noptimization, it will not cause any influence to the deep neural features. The capability, functionality,\nand result quality of any neural network block is perfectly preserved, and any further optimization\nwill become as fast as fine tuning (compared to train those layers from scratch).\nWe briefly deduce the gradient calculation of a zero convolution layer. Considering an 1 \u00d7 1\nconvolution layer with weight W and bias B, at any spatial position p and channel-wise index i,\ngiven an input map I \u2208 R h\u00d7w\u00d7c , the forward pass can be written as\nZ(I; {W , B}) p,i = B i +\nc\nX\nI p,i W i,j\n(5)\nj\nand since zero convolution has W = 0 and B = 0 (before optimization), for anywhere with I p,i\nbeing non-zero, the gradients become\n\uf8f1\n\u2202Z(I; {W , B}) p,i\n\uf8f4\n\uf8f4\n=1\n\uf8f4\n\uf8f4\n\u2202B i\n\uf8f4\n\uf8f4\n\uf8f4\nc\n\uf8f4\nX\n\uf8f2 \u2202Z(I; {W , B}) p,i\n=\nW i,j = 0\n(6)\n\u2202I p,i\n\uf8f4\n\uf8f4\nj\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\u2202Z(I; {W , B}) p,i\n\uf8f4\n\uf8f4\n= I p,i 6 = 0\n\uf8f3\n\u2202W i,j\nand we can see that although a zero convolution can cause the gradient on the feature term I to\nbecome zero, the weight\u2019s and bias\u2019s gradients are not influenced. As long as the feature I is non-zero,\nthe weight W will be optimized into non-zero matrix in the first gradient descent iteration. Notably,\nin our case, the feature term is input data or condition vectors sampled from datasets, which naturally\nensures non-zero I. For example, considering a classic gradient descent with an overall loss function\nL and a learning rate \u03b2 lr 6 = 0, if the \u201coutside\u201d gradient \u2202L/\u2202Z(I; {W , B}) is not zero, we will have\n\u2202L\n\u2202Z(I; {W , B})\n= 0\n(7)\n\u2202Z(I; {W , B})\n\u2202W\nwhere W \u2217 is the weight after one gradient descent step; is Hadamard product. After this step, we\nwill have\nc\nX\n\u2202Z(I; {W \u2217 , B}) p,i\n\u2217\n=\nW i,j\n= 0\n(8)\nwhere non-zero gradients are obtained and the neural network begins to learn. In this way, the\nzero convolutions become an unique type of connection layer that progressively grow from zeros to\noptimized parameters in a learned way.",
    "p9.txt": "3.2\nControlNet in Image Diffusion Model\nWe use the Stable Diffusion [44] as an example to introduce the method to use ControlNet to control\na large diffusion model with task-specific conditions.\nStable Diffusion is a large text-to-image diffusion model trained on billions of images. The model\nis essentially an U-net with an encoder, a middle block, and a skip-connected decoder. Both the\nencoder and decoder have 12 blocks, and the full model has 25 blocks (including the middle block).\nIn those blocks, 8 blocks are down-sampling or up-sampling convolution layers, 17 blocks are main\nblocks that each contains four resnet layers and two Vision Transformers (ViTs). Each Vit contains\nseveral cross-attention and/or self-attention mechanisms. The texts are encoded by OpenAI CLIP,\nand diffusion time steps are encoded by positional encoding.\nStable Diffusion uses a pre-processing method similar to VQ-GAN [11] to convert the entire dataset\nof 512 \u00d7 512 images into smaller 64 \u00d7 64 \u201clatent images\u201d for stabilized training. This requires\nControlNets to convert image-based conditions to 64 \u00d7 64 feature space to match the convolution\nsize. We use a tiny network E(\u00b7) of four convolution layers with 4 \u00d7 4 kernels and 2 \u00d7 2 strides\n(activated by ReLU, channels are 16, 32, 64, 128, initialized with Gaussian weights, trained jointly\nwith the full model) to encode image-space conditions c i into feature maps with\nc f = E(c i )\n(9)where c f is the converted feature map. This network convert 512 \u00d7 512 image conditions to 64 \u00d7 64\nfeature maps.\nAs shown in Fig. 3, we use ControlNet to control each level of the U-net. Note that the way we\nconnect the ControlNet is computationally efficient: since the original weights are locked, no gradient\ncomputation on the original encoder is needed for training. This can speed up training and save GPU\nmemory, as half of the gradient computation on the original model can be avoided. Training a stable\ndiffusion model with ControlNet requires only about 23% more GPU memory and 34% more time in\neach training iteration (as tested on a single Nvidia A100 PCIE 40G).\nTo be specific, we use ControlNet to create the trainable copy of the 12 encoding blocks and 1 middle\nblock of Stable Diffusion. The 12 blocks are in 4 resolutions (64 \u00d7 64, 32 \u00d7 32, 16 \u00d7 16, 8 \u00d7 8) with\neach having 3 blocks. The outputs are added to the 12 skip-connections and 1 middle block of the\nU-net. Since SD is a typical U-net structure, this ControlNet architecture is likely to be usable in\nother diffusion models.",
    "p10.txt": "3.3\nTraining\nImage diffusion models learn to progressively denoise images to generate samples. The denoising\ncan happen in pixel space or a \u201clatent\u201d space encoded from training data. Stable Diffusion uses latent\nimages as the training domain. In this context, the terminology \u201cimage\u201d, \u201dpixel\u201d, and \u201cdenoising\u201d all\nrefers to corresponding concepts in the \u201cperceptual latent space\u201d [44].\nGiven an image z 0 , diffusion algorithms progressively add noise to the image and produces a noisy\nimage z t , with t being how many times the noise is added. When t is large enough, the image\napproximates pure noise. Given a set of conditions including time step t, text prompts c t , as well as a\ntask-specific conditions c f , image diffusion algorithms learn a network \u000f \u03b8 to predict the noise added\nto the noisy image z t with\nh\ni\nL = E z 0 ,t,c t ,c f ,\u000f\u223cN (0,1) k\u000f \u2212 \u000f \u03b8 (z t , t, c t , c f ))k 22\n(10)\nwhere L is the overall learning objective of the entire diffusion model. This learning objective can be\ndirectly used in fine tuning diffusion models.\nDuring the training, we randomly replace 50% text prompts c t with empty strings. This facilitates\nControlNet\u2019s capability to recognize semantic contents from input condition maps, e.g., Canny edge\nmaps or human scribbles, etc. This is mainly because when the prompt is not visible for the SD\nmodel, the encoder tends to learn more semantics from input control maps as a replacement for the\nprompt.",
    "p11.txt": "3.4\nImproved Training\nWe discuss several strategies to improve the training of ControlNets, especially in extreme cases when\nthe computation device is very limited (e.g., on a laptop) or very powerful (e.g., on a computation\ncluster with large-scale GPUs available). In our experiments, if any of these strategies are used, we\nwill mention in the experimental settings.\nSmall-Scale Training When computation device is limited, we find that partially breaking the\nconnection between a ControlNet and the Stable Diffusion can accelerate convergence. By default,\nwe connect the ControlNet to \u201cSD Middle Block\u201d and \u201cSD Decoder Block 1,2,3,4\u201d as shown in\nFig. 3. We find that disconnecting the link to decoder 1,2,3,4 and only connecting the middle block\ncan improve the training speed by about a factor of 1.6 (tested on RTX 3070TI laptop GPU). When\nthe model shows reasonable association between results and conditions, those disconnected links can\nbe connected again in a continued training to facilitate accurate control.\nLarge-Scale Training Herein, the large-scale training refers to the situation where both powerful\ncomputation clusters (at least 8 Nvidia A100 80G or equivalent) and large dataset (at least 1 million\nof training image pairs) are available. This usually applies to tasks where data is easily available, e.g.,\nedge maps detected by Canny. In this case, since the risk of over-fitting is relatively low, we can first\ntrain ControlNets for a large enough number of iterations (usually more than 50k steps), and then\nunlock all weights of the Stable Diffusion and jointly train the entire model as a whole. This would\nlead to a more problem-specific model.",
    "p12.txt": "3.5\nImplementation\nWe present several implementations of ControlNets with different image-based conditions to control\nlarge diffusion models in various ways.\nCanny Edge We use Canny edge detector [5] (with random thresholds) to obtain 3M edge-image-\ncaption pairs from the internet. The model is trained with 600 GPU-hours with Nvidia A100 80G.\nThe base model is Stable Diffusion 1.5. (See also Fig. 4.)\nCanny Edge (Alter) We rank the image resolutions of the above Canny edge dataset and sampled\nseveral sub-set with 1k, 10k, 50k, 500k samples. We use the same experimental setting to test the\neffect of dataset scale. (See also Fig. 22.)\nHough Line We use a learning-based deep Hough transform [13] to detect straight lines from\nPlaces2 [66], and then use BLIP [34] to generate captions. We obtain 600k edge-image-caption pairs.\nWe use the above Canny model as a starting checkpoint and train the model with 150 GPU-hours\nwith Nvidia A100 80G. (See also Fig. 5.)\nHED Boundary We use HED boundary detection [62] to obtain 3M edge-image-caption pairs\nfrom internet. The model is trained with 300 GPU-hours with Nvidia A100 80G. The base model is\nStable Diffusion 1.5. (See also Fig. 7.)\nUser Sketching We synthesize human scribbles from images using a combination of HED boundary\ndetection [62] and a set of strong data augmentations (random thresholds, randomly masking out a\nrandom percentage of scribbles, random morphological transformations, and random non-maximum\nsuppression). We obtain 500k scribble-image-caption pairs from internet. We use the above Canny\nmodel as a starting checkpoint and train the model with 150 GPU-hours with Nvidia A100 80G. Note\nthat we also tried a more \u201chuman-like\u201d synthesizing method [57] but the method is much slower than\na simple HED and we do not notice visible improvements. (See also Fig. 6.)\nHuman Pose (Openpifpaf) We use learning-based pose estimation method [27] to \u201cfind\u201d humans\nfrom internet using a simple rule: an image with human must have at least 30% of the key points\nof the whole body detected. We obtain 80k pose-image-caption pairs. Note that we directly use\nvisualized pose images with human skeletons as training condition. The model is trained with 400\nGPU-hours on Nvidia RTX 3090TI. The base model is Stable Diffusion 2.1. (See also Fig. 8.)\nHuman Pose (Openpose) We use learning-based pose estimation method [6] to find humans from\ninternet using the same rule in the above Openpifpaf setting. We obtain 200k pose-image-caption\npairs. Note that we directly use visualized pose images with human skeletons as training condition.\nThe model is trained with 300 GPU-hours with Nvidia A100 80G. Other settings are same with the\nabove Openpifpaf. (See also Fig. 9.)\nSemantic Segmentation (COCO) The COCO-Stuff dataset [4] captioned by BLIP [34]. We obtain\n164K segmentation-image-caption pairs. The model is trained with 400 GPU-hours on Nvidia RTX\n3090TI. The base model is Stable Diffusion 1.5. (See also Fig. 12.)\nSemantic Segmentation (ADE20K) The ADE20K dataset [67] captioned by BLIP [34]. We\nobtain 164K segmentation-image-caption pairs. The model is trained with 200 GPU-hours on Nvidia\nA100 80G. The base model is Stable Diffusion 1.5. (See also Fig. 11.)\nDepth (large-scale) We use the Midas [30] to obtain 3M depth-image-caption pairs from internet.\nThe model is trained with 500 GPU-hours with Nvidia A100 80G. The base model is Stable Diffusion\n1.5. (See also Fig. 23,24,25.)\nDepth (small-scale) We rank the image resolutions of the above depth dataset to sample a subset\nof 200k pairs. This set is used in experimenting the minimal required dataset size to train the model.\n(See also Fig. 14.)\n8Normal Maps The DIODE dataset [56] captioned by BLIP [34]. We obtain 25,452 normal-image-\ncaption pairs. The model is trained with 100 GPU-hours on Nvidia A100 80G. The base model is\nStable Diffusion 1.5. (See also Fig. 13.)\nNormal Maps (extended) We use the Midas [30] to compute depth map and then perform normal-\nfrom-distance to achieve \u201ccoarse\u201d normal maps. We use the above Normal model as a starting\ncheckpoint and train the model with 200 GPU-hours with Nvidia A100 80G. (See also Fig. 23,24,25.)\nCartoon Line Drawing We use a cartoon line drawing extracting method [61] to extract line\ndrawings from cartoon illustration from internet. By sorting the cartoon images with popularity, we\nobtain the top 1M lineart-cartoon-caption pairs. The model is trained with 300 GPU-hours with\nNvidia A100 80G. The base model is Waifu Diffusion (an interesting community-developed variation\nmodel from stable diffusion [36]). (See also Fig. 15.)",
    "p13.txt": "4\n4.1\nExperiment\nExperimental Settings\nAll results in this paper is achieved with CFG-scale at 9.0. The sampler is DDIM. We use 20 steps by\ndefault. We use three types of prompts to test the models:\n(1) No prompt: We use empty string \u201c\u201d as prompt.\n(2) Default prompt: Since Stable diffusion is essentially trained with prompts, the empty string might\nbe an unexpected input for the model, and SD tends to generate random texture maps if no prompt\nis provided. A better setting is to use meaningless prompts like \u201can image\u201d, \u201ca nice image\u201d, \u201ca\nprofessional image\u201d, etc. In our setting, we use \u201ca professional, detailed, high-quality image\u201d as\ndefault prompt.\n(3) Automatic prompt: In order to test the state-of-the-art maximized quality of a fully automatic\npipeline, we also try to use automatic image captioning methods (e.g., BLIP [34]) to generate prompts\nusing the results obtained by \u201cdefault prompt\u201d mode. We use the generated prompt to diffusion again.\n(4) User prompt: Users give the prompts.",
    "p14.txt": "4.2\nQualitative Results\nWe present qualitative results in Fig. 4, 5,6,7,8,9,10,11,12,13,14,15.",
    "p15.txt": "4.3\nAblation Study\nFig. 20 shows a comparison to a model trained without using ControlNet. That model is trained\nwith exactly same method with Stability\u2019s Depth-to-Image model (Adding a channel to the SD and\ncontinue the training).\nFig. 21 shows the training process. We would like to point out a \u201csudden convergence phenomenon\u201d\nwhere the model suddenly be able to follow the input conditions. This can happen during the training\nprocess from 5000 to 10000 steps when using 1e-5 as the learning rate.\nFig. 22 shows Canny-edge-based ControlNets trained with different dataset scales.",
    "p16.txt": "4.4\nComparison to previous methods\nFig. 14 shows the comparison to Stability\u2019s Depth-to-Image model.\nFig. 17 shows a comparison to PITI [59].\nFig. 18 shows a comparison to sketch-guided diffusion [58].\nFig. 19 shows a comparison to Taming transformer [11].\n94.5\nComparison of pre-trained models\nWe show comparisons of different pre-trained models in Fig. 23, 24, 25.",
    "p17.txt": "4.5\nComparison of pre-trained models\nWe show comparisons of different pre-trained models in Fig. 23, 24, 25.",
    "p18.txt": "4.6\nMore Applications\nFig. 16 show that if the diffusion process is masked, the models can be used in pen-based image\nediting.\nFig. 26 show that when object is relatively simple, the model can achieve relatively accurate control\nof the details.\nFig. 27 shows that when ControlNet is only applied to 50% diffusion iterations, users can get results\nthat do not follow the input shapes."
}